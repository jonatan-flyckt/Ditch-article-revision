{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,recall_score,confusion_matrix, cohen_kappa_score, precision_score\n",
    "from sklearn.metrics import precision_recall_curve, auc, average_precision_score, f1_score, roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from joblib import dump, load\n",
    "import random\n",
    "import pickle\n",
    "import scipy.stats\n",
    "from PIL import Image\n",
    "import scipy.stats as stats\n",
    "\n",
    "from Functions import post_processing\n",
    "from Functions import general_functions\n",
    "from Functions import feature_creation\n",
    "\n",
    "from skimage.restoration import denoise_bilateral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads all zones with their features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f\"../zones_features_final/zone_{i}.pickle\" for i in range(11, 22)]\n",
    "importances = np.zeros(0)\n",
    "predictions = np.zeros(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following feature set was identified as the optimal in a tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = ['impundment_mean_3', 'impundment_mean_4', 'impundment_median_4', 'impundment_mean_2',\n",
    "                   'impundment_amplified', 'hpmf_mean_4', 'impoundment_amplified_no_streams', 'hpmf_median_4',\n",
    "                   'skyview_max_6', 'skyview_gabor', 'skyview_non_ditch', 'slope_min_6', 'skyview_max_4',\n",
    "                   'skyview_gabor_no_streams', 'impundment_std_4', 'impundment_max_6', 'slope_non_ditch', 'hpmf_filter',\n",
    "                   'hpmf_mean_3', 'hpmf_filter_no_streams', 'hpmf_mean_6', 'impundment_median_6', 'impundment_median_2',\n",
    "                   'slope_std_6', 'impundment_mean_6', 'slope_median_6', 'slope_min_4', 'hpmf_min_2', 'impundment_std_6',\n",
    "                   'skyview_median_6', 'skyview_min_6', 'slope_mean_6', 'impundment_max_2', 'skyview_std_6', 'slope_min_2',\n",
    "                   'skyview_max_2', 'hpmf_min_4', 'impundment_max_4', 'slope_std_4', 'hpmf_gabor', 'hpmf_std_6',\n",
    "                   'slope_max_4', 'hpmf_gabor_no_streams', 'skyview_min_4', 'slope_median_4', 'hpmf_mean_2', 'slope_median_2',\n",
    "                   'slope_std_2', 'hpmf_min_6', 'hpmf_max_6', 'skyview_mean_6', 'skyview_median_4', 'hpmf_std_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following parameters were identified as the optimal in a tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion': 'gini',\n",
    "   'max_depth': None,\n",
    "   'max_features': 'sqrt',\n",
    "   'min_samples_split': 10,\n",
    "   'class_weight': None,\n",
    "   'n_estimators': 300}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performs 11 sub-experiments\n",
    "Trains with 10 zones for each sub-experiment, and evaluates on one zone.\n",
    "Each zone is evaluated once. Similar to cross validation but without randomly assigned subsets of data.\n",
    "All of these 11 zones were hold-out data, which means that they had never before been used in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Skola\\Examensarbete\\Ditch-article-revision\\Functions\\post_processing.py:116: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"custom_remove_noise\" failed type inference due to: \u001b[1mUntyped global name 'd_gf':\u001b[0m \u001b[1m\u001b[1mcannot determine Numba type of <class 'function'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"Functions\\post_processing.py\", line 121:\u001b[0m\n",
      "\u001b[1mdef custom_remove_noise(arr, radius, threshold, selfThreshold):\n",
      "    <source elided>\n",
      "    \"\"\"\n",
      "\u001b[1m    max_arr = d_gf(da.from_array(arr, chunks=(800, 800)), np.nanmax,\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "C:\\Users\\Jonatan\\.conda\\envs\\pytorch\\lib\\site-packages\\numba\\core\\object_mode_passes.py:177: NumbaWarning: \u001b[1mFunction \"custom_remove_noise\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"Functions\\post_processing.py\", line 117:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef custom_remove_noise(arr, radius, threshold, selfThreshold):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "C:\\Users\\Jonatan\\.conda\\envs\\pytorch\\lib\\site-packages\\numba\\core\\object_mode_passes.py:187: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"Functions\\post_processing.py\", line 117:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef custom_remove_noise(arr, radius, threshold, selfThreshold):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n",
      "E:\\Skola\\Examensarbete\\Ditch-article-revision\\Functions\\general_functions.py:51: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"create_circular_mask\" failed type inference due to: \u001b[1m\u001b[1mUse of unsupported NumPy function 'numpy.ogrid' or unsupported use of the function.\n",
      "\u001b[1m\n",
      "File \"Functions\\general_functions.py\", line 57:\u001b[0m\n",
      "\u001b[1mdef create_circular_mask(radius):\n",
      "    <source elided>\n",
      "    kernel = np.zeros((2*radius+1, 2*radius+1))\n",
      "\u001b[1m    y, x = np.ogrid[-radius:radius+1, -radius:radius+1]\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: typing of get attribute at E:\\Skola\\Examensarbete\\Ditch-article-revision\\Functions\\general_functions.py (57)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"Functions\\general_functions.py\", line 57:\u001b[0m\n",
      "\u001b[1mdef create_circular_mask(radius):\n",
      "    <source elided>\n",
      "    kernel = np.zeros((2*radius+1, 2*radius+1))\n",
      "\u001b[1m    y, x = np.ogrid[-radius:radius+1, -radius:radius+1]\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "C:\\Users\\Jonatan\\.conda\\envs\\pytorch\\lib\\site-packages\\numba\\core\\object_mode_passes.py:177: NumbaWarning: \u001b[1mFunction \"create_circular_mask\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"Functions\\general_functions.py\", line 52:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef create_circular_mask(radius):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "C:\\Users\\Jonatan\\.conda\\envs\\pytorch\\lib\\site-packages\\numba\\core\\object_mode_passes.py:187: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"Functions\\general_functions.py\", line 52:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef create_circular_mask(radius):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n",
      "E:\\Skola\\Examensarbete\\Ditch-article-revision\\Functions\\post_processing.py:323: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"conic_proba_post_processing\" failed type inference due to: \u001b[1mUntyped global name 'd_gf':\u001b[0m \u001b[1m\u001b[1mcannot determine Numba type of <class 'function'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"Functions\\post_processing.py\", line 329:\u001b[0m\n",
      "\u001b[1mdef conic_proba_post_processing(arr, maskRadius, threshold):\n",
      "    <source elided>\n",
      "    masks = []\n",
      "\u001b[1m    maxArr = d_gf(da.from_array(arr,chunks = (800,800)), np.nanmax, footprint=general_functions.create_circular_mask(5))\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "E:\\Skola\\Examensarbete\\Ditch-article-revision\\Functions\\post_processing.py:323: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"conic_proba_post_processing\" failed type inference due to: \u001b[1mUntyped global name 'd_gf':\u001b[0m \u001b[1m\u001b[1mcannot determine Numba type of <class 'function'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"Functions\\post_processing.py\", line 329:\u001b[0m\n",
      "\u001b[1mdef conic_proba_post_processing(arr, maskRadius, threshold):\n",
      "    <source elided>\n",
      "    masks = []\n",
      "\u001b[1m    maxArr = d_gf(da.from_array(arr,chunks = (800,800)), np.nanmax, footprint=general_functions.create_circular_mask(5))\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "C:\\Users\\Jonatan\\.conda\\envs\\pytorch\\lib\\site-packages\\numba\\core\\object_mode_passes.py:177: NumbaWarning: \u001b[1mFunction \"conic_proba_post_processing\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"Functions\\post_processing.py\", line 328:\u001b[0m\n",
      "\u001b[1mdef conic_proba_post_processing(arr, maskRadius, threshold):\n",
      "    <source elided>\n",
      "    \"\"\"\n",
      "\u001b[1m    masks = []\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "C:\\Users\\Jonatan\\.conda\\envs\\pytorch\\lib\\site-packages\\numba\\core\\object_mode_passes.py:187: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"Functions\\post_processing.py\", line 328:\u001b[0m\n",
      "\u001b[1mdef conic_proba_post_processing(arr, maskRadius, threshold):\n",
      "    <source elided>\n",
      "    \"\"\"\n",
      "\u001b[1m    masks = []\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n",
      "E:\\Skola\\Examensarbete\\Ditch-article-revision\\Functions\\post_processing.py:323: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"conic_proba_post_processing\" failed type inference due to: \u001b[1mUntyped global name '_create_conic_mask':\u001b[0m \u001b[1m\u001b[1mcannot determine Numba type of <class 'numba.core.ir.UndefinedType'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"Functions\\post_processing.py\", line 331:\u001b[0m\n",
      "\u001b[1mdef conic_proba_post_processing(arr, maskRadius, threshold):\n",
      "    <source elided>\n",
      "    for i in range(0, 8):\n",
      "\u001b[1m        masks.append(_create_conic_mask(maskRadius, i))\n",
      "\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "C:\\Users\\Jonatan\\.conda\\envs\\pytorch\\lib\\site-packages\\numba\\core\\object_mode_passes.py:177: NumbaWarning: \u001b[1mFunction \"conic_proba_post_processing\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"Functions\\post_processing.py\", line 330:\u001b[0m\n",
      "\u001b[1mdef conic_proba_post_processing(arr, maskRadius, threshold):\n",
      "    <source elided>\n",
      "    maxArr = d_gf(da.from_array(arr,chunks = (800,800)), np.nanmax, footprint=general_functions.create_circular_mask(5))\n",
      "\u001b[1m    for i in range(0, 8):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "C:\\Users\\Jonatan\\.conda\\envs\\pytorch\\lib\\site-packages\\numba\\core\\object_mode_passes.py:187: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"Functions\\post_processing.py\", line 330:\u001b[0m\n",
      "\u001b[1mdef conic_proba_post_processing(arr, maskRadius, threshold):\n",
      "    <source elided>\n",
      "    maxArr = d_gf(da.from_array(arr,chunks = (800,800)), np.nanmax, footprint=general_functions.create_circular_mask(5))\n",
      "\u001b[1m    for i in range(0, 8):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name '_create_conic_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6aef41ed72b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2997\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2620\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mproba_post_process\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpost_processing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproba_post_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mlabels_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpost_processing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraster_to_zones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2997\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2620\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Skola\\Examensarbete\\Ditch-article-revision\\Functions\\post_processing.py\u001b[0m in \u001b[0;36mproba_post_process\u001b[1;34m(arr, zoneSize, probaThreshold)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \"\"\"\n\u001b[0;32m    353\u001b[0m     \u001b[0mdeNoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproba_noise_reduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m     \u001b[0mgapFilled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconic_proba_post_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconic_proba_post_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeNoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.35\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m     \u001b[0mzonesArr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproba_to_zones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgapFilled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzoneSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[0mnoIslands\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_clusters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzonesArr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzoneSize\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name '_create_conic_mask' is not defined"
     ]
    }
   ],
   "source": [
    "i = 11\n",
    "for (training_files, test_file) in general_functions.yield_training_test_zones(file_list):\n",
    "    training_dataset = general_functions.create_balanced_dataset(training_files)\n",
    "    \n",
    "    X_train = training_dataset.loc[:, training_dataset.columns != \"label_3m\"]\n",
    "    X_train = X_train.filter(items=features_to_use)\n",
    "    y_train = training_dataset[\"label_3m\"]\n",
    "    training_dataset = None\n",
    "    \n",
    "    clf = RandomForestClassifier(**params, n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    #dump(clf, f\"../revised_results/classifier_for_zone_{i}.joblib\")\n",
    "    i += 1\n",
    "    \n",
    "    test_dataset = pd.read_pickle(test_file)\n",
    "    X_test = test_dataset.loc[:, test_dataset.columns != \"label_3m\"]\n",
    "    X_test = X_test.filter(items=features_to_use)\n",
    "    y_test = test_dataset[\"label_3m\"]\n",
    "    \n",
    "    proba = clf.predict_proba(X_test)[:,1:].reshape(2997,2620)\n",
    "    \n",
    "    proba_post_process = post_processing.proba_post_process(proba, 6, 0.4)\n",
    "    \n",
    "    labels_grid = post_processing.raster_to_zones(y_test.reshape(2997, 2620), 6, 4)\n",
    "    np.append(predictions, (proba, proba_post_process.reshape(-1), labels_grid.reshape(-1)))\n",
    "    \n",
    "    np.save(\"../revised_results/predictions.npy\", predictions)\n",
    "    \n",
    "    importances = clf.feature_importances_\n",
    "    feature_names = features_to_use\n",
    "    tuple_features = [(feature_names[i], importance) for i, importance in enumerate(importances)]\n",
    "    experiment_importances = np.zeros(len(X_test.shape[1]))\n",
    "    for f in range(X_test.shape[1]):\n",
    "        np.append(experiment_importances, (tuple_features[f][0], tuple_features[f][1]*100))\n",
    "    np.append(importances, experiment_importances)\n",
    "    np.save(\"../revised_results/feature_importances_11_experiments.npy\", importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads saved experiment results from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.load(\"../revised_results/predictions.npy\")\n",
    "prediction2d = np.concatenate(predictions, axis= 1)\n",
    "y_pred_all = prediction2d[1]\n",
    "y_test_all = prediction2d[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.reshape(11, 3, 2997, 2620)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying evaluation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(predictions.shape[0]):\n",
    "    pred = predictions[i, 1]\n",
    "    labels = predictions[i, 2]\n",
    "    new_labels = labels.copy()\n",
    "    false_positives = np.zeros(pred.shape)\n",
    "    false_negatives = np.zeros(pred.shape)\n",
    "    for j in range(pred.shape[0]):\n",
    "        for k in range(pred.shape[1]):\n",
    "            if pred[j, k] == 1 and labels[j, k] == 0:\n",
    "                false_positives[j, k] = 1\n",
    "            elif pred[j, k] == 0 and labels[j, k] == 1:\n",
    "                false_negatives[j, k] = 1\n",
    "    for j in range(0, len(pred) - 6, 6):\n",
    "        for k in range(0, len(pred[i]) - 6, 6):\n",
    "            if false_positives[j, k] == 1:\n",
    "                if labels[j+6, k] == 1 or labels[j-6, k] == 1 or labels[j, k+6] == 1 or labels[j, k-6] == 1:\n",
    "                    for l in range(j, j+6):\n",
    "                        for m in range(k, k+6):\n",
    "                            new_labels[l, m] = 1\n",
    "            if false_negatives[j, k] == 1:\n",
    "                if pred[j+6, k] == 1 or pred[j-6, k] == 1 or pred[j, k+6] == 1 or pred[j, k-6] == 1:\n",
    "                    for l in range(j, j+6):\n",
    "                        for m in range(k, k+6):\n",
    "                            new_labels[l, m] = 0\n",
    "                            \n",
    "    np.save(f\"../revised_results/new_labels_zone_{i+1}.npy\", new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.load(\"../revised_results/predictions.npy\")\n",
    "prediction2d = np.concatenate(predictions, axis= 1)\n",
    "y_pred_all = prediction2d[1]\n",
    "y_test_all = np.load(\"../revised_results/new_labels_zone_1.npy\").reshape(-1)\n",
    "for i in range (2, 12):\n",
    "    y_test_all = np.append(y_test_all, np.load(f\"../revised_results/new_labels_zone_{i}.npy\").reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score             \", accuracy_score(y_test_all, y_pred_all))\n",
    "print(\"Recall score               \", recall_score(y_test_all, y_pred_all))\n",
    "print(\"Precision score            \", precision_score(y_test_all, y_pred_all))\n",
    "precision, recall, threshholds = precision_recall_curve(y_test_all,y_pred_all)\n",
    "auc_score = auc(recall, precision)\n",
    "print(\"Cohen's kappa score        \", cohen_kappa_score(y_test_all, y_pred_all))\n",
    "print(\"AUPRC score                \", auc_score)\n",
    "print(\"Confusion matrix\\n\", confusion_matrix(y_test_all, y_pred_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_zone = []\n",
    "for i, zone in enumerate(predictions):\n",
    "    proba, binary_prediction, old_labels = zone\n",
    "    labels_grid = np.load(f\"../revised_results/new_labels_zone_{i+1}.npy\").reshape(-1)\n",
    "    acc = accuracy_score(labels_grid, binary_prediction)\n",
    "    _recall = recall_score(labels_grid, binary_prediction)\n",
    "    _precision = precision_score(labels_grid, binary_prediction)\n",
    "    kappa = cohen_kappa_score(labels_grid, binary_prediction)\n",
    "    precision, recall, threshholds = precision_recall_curve(labels_grid, binary_prediction)\n",
    "    auc_score = auc(recall, precision)\n",
    "    metrics = (acc, _recall, _precision, kappa, auc_score)\n",
    "    metrics_per_zone.append(metrics)\n",
    "    print(f\"Zone {i+1}:\\nAccuracy: {metrics[0]}, Recall: {metrics[1]}, Precision: {metrics[2]}, Cohen's Kappa: {metrics[3]}, AUPRC: {metrics[4]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence = 0.95):\n",
    "    sample = np.array(data)\n",
    "    sample_size, sample_mean = len(sample), sample.mean()\n",
    "    z_critical = stats.norm.ppf(q = ((1+confidence) / 2.))  # Get the z-critical value*\n",
    "    margin_of_error = z_critical * (sample.std() / math.sqrt(sample_size))\n",
    "    return (sample_mean, sample_mean - margin_of_error, sample_mean + margin_of_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"accuracy\", \"recall\", \"precision\", \"Cohen's Kappa\", \"AUPRC\"]\n",
    "avg = 0\n",
    "for i in range(0, 5):\n",
    "    for interval in [0.90, 0.95, 0.99]:\n",
    "        conf_int = mean_confidence_interval([j[i] for j in metrics_per_zone], interval)\n",
    "        avg = conf_int[0]\n",
    "        print(f\"Confidence interval for {metric_names[i]} at {interval * 100}%: [{conf_int[1]}, {conf_int[2]}]\")\n",
    "    print(f\"Average for {metric_names[i]}: {avg}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(15,65), facecolor='w', edgecolor='k')\n",
    "ax = [plt.subplot(11,3,i+1) for i in range(33)]\n",
    "for a in ax:\n",
    "    a.set_xticklabels([])\n",
    "    a.set_yticklabels([])\n",
    "    a.tick_params(bottom=False, left=False)\n",
    "plt.subplots_adjust(wspace=0,hspace=0.1)\n",
    "\n",
    "for i, zone_pred in enumerate(predictions):\n",
    "    ax[i*3].title.set_text(f\"Raw probability zone {i+1}\")\n",
    "    ax[i*3].imshow(zone_pred[0].reshape(2997,2620))\n",
    "    ax[i*3+1].title.set_text(f\"Binary prediction zone {i+1}\")\n",
    "    ax[i*3+1].imshow(zone_pred[1].reshape(2997,2620))\n",
    "    ax[i*3+2].title.set_text(f\"Labels zone {i+1}\")\n",
    "    ax[i*3+2].imshow(np.load(f\"../revised_results/new_labels_zone_{i+1}.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(30,45), facecolor='w', edgecolor='k')\n",
    "ax = [plt.subplot(4,3,i+1) for i in range(11)]\n",
    "for a in ax:\n",
    "    a.set_xticklabels([])\n",
    "    a.set_yticklabels([])\n",
    "    a.tick_params(bottom=False, left=False)\n",
    "plt.subplots_adjust(wspace=0,hspace=0.1)\n",
    "\n",
    "for k, zone_pred in enumerate(predictions):\n",
    "    validation = np.load(f\"../revised_results/new_labels_zone_{k+1}.npy\")\n",
    "    pred = zone_pred[1].reshape(2997,2620)\n",
    "    displayImg = Image.new(\"RGB\", (2620, 2997), \"white\")\n",
    "    pixels = displayImg.load()\n",
    "    for i in range(displayImg.size[0]):\n",
    "        for j in range(displayImg.size[1]):\n",
    "            if validation[j][i] == 1 and pred[j][i] == 1:\n",
    "                pixels[i,j] = (0, 180, 0)\n",
    "            elif validation[j][i] == 1 and pred[j][i] == 0:\n",
    "                pixels[i,j] = (50, 150, 255)\n",
    "            elif validation[j][i] == 0 and pred[j][i] == 1:\n",
    "                pixels[i,j] = (255, 50, 50)\n",
    "    \n",
    "    ax[k].title.set_text(f\"Result graphics zone {k+1}\")\n",
    "    ax[k].imshow(displayImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
